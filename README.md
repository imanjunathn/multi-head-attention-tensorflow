## Multi Head Attention Tensorflow

This repository contains the implementation of **Multi Head attention** from scratch using Tensorflow and Keras.
It includes the compoents of attention mechanism, particularly **Scaled Dot Product Attention** and **Multi Head Attention**, which are crusial for transformer models.

## Features
1. Dot Product Attention: Computes attention scores using query, key, and value.
2. Multi-Head Attention: Allows the model to focus on different parts of the sequence by splitting the input into multiple heads.

