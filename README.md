## Multi Head Attention Tensorflow

This repository contains the implementation of **Multi Head attention** from scratch using Tensorflow and Keras.
It includes the compoents of attention mechanism, particularly **Scaled Dot Product Attention** and **Multi Head Attention**, which are crusial for transformer models.

## Features
Dot Product Attention: Computes attention scores using query, key, and value.
Multi-Head Attention: Allows the model to focus on different parts of the sequence by splitting the input into multiple heads.

