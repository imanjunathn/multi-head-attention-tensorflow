{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6eaeebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:57:18.627149Z",
     "iopub.status.busy": "2024-11-14T09:57:18.626720Z",
     "iopub.status.idle": "2024-11-14T09:57:34.695335Z",
     "shell.execute_reply": "2024-11-14T09:57:34.694121Z"
    },
    "papermill": {
     "duration": 16.07616,
     "end_time": "2024-11-14T09:57:34.698278",
     "exception": false,
     "start_time": "2024-11-14T09:57:18.622118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9544f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:57:34.705620Z",
     "iopub.status.busy": "2024-11-14T09:57:34.704947Z",
     "iopub.status.idle": "2024-11-14T09:57:34.714667Z",
     "shell.execute_reply": "2024-11-14T09:57:34.713296Z"
    },
    "papermill": {
     "duration": 0.016072,
     "end_time": "2024-11-14T09:57:34.717263",
     "exception": false,
     "start_time": "2024-11-14T09:57:34.701191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention,self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):   \n",
    "        dk = tf.cast(tf.shape(k)[-1],tf.float32)\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        scores = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores += (-1e9 * mask)\n",
    "        \n",
    "        weights = tf.nn.softmax(scores)\n",
    "            \n",
    "        return tf.matmul(weights, v)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c006e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:57:34.723833Z",
     "iopub.status.busy": "2024-11-14T09:57:34.723351Z",
     "iopub.status.idle": "2024-11-14T09:57:34.735639Z",
     "shell.execute_reply": "2024-11-14T09:57:34.734130Z"
    },
    "papermill": {
     "duration": 0.018513,
     "end_time": "2024-11-14T09:57:34.738172",
     "exception": false,
     "start_time": "2024-11-14T09:57:34.719659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, d_model, number_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = number_heads\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        \n",
    "        assert self.d_model % self.num_heads == 0, \"d_model must be divisible by number_heads\"\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.wq = Dense(self.d_model)\n",
    "        self.wk = Dense(self.d_model)\n",
    "        self.wv = Dense(self.d_model)\n",
    "        self.dense = Dense(self.d_model)\n",
    "        \n",
    "    def  reshape_tensor(self,x, batch_size, flag=False):\n",
    "        if flag:\n",
    "            x = tf.reshape(x,(batch_size, -1, self.num_heads, self.depth))\n",
    "            return tf.transpose(x,perm=(0,2,1,3))\n",
    "        else:    \n",
    "            x = tf.transpose(x,perm=(0,2,1,3))\n",
    "            x = tf.reshape(x, (batch_size, -1, self.d_model))\n",
    "            return x\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        wq = self.wq(q)\n",
    "        wk = self.wq(k)\n",
    "        wv = self.wq(v)\n",
    "        \n",
    "        q = self.reshape_tensor(wq,batch_size,True)\n",
    "        k = self.reshape_tensor(wk,batch_size,True)\n",
    "        v = self.reshape_tensor(wv,batch_size,True)\n",
    "        \n",
    "        scaled_dot_product = self.attention(q, k, v)\n",
    "        \n",
    "        output = self.reshape_tensor(scaled_dot_product, batch_size)\n",
    "        \n",
    "        return self.dense(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab05a497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:57:34.744960Z",
     "iopub.status.busy": "2024-11-14T09:57:34.744498Z",
     "iopub.status.idle": "2024-11-14T09:57:35.030125Z",
     "shell.execute_reply": "2024-11-14T09:57:35.028890Z"
    },
    "papermill": {
     "duration": 0.294039,
     "end_time": "2024-11-14T09:57:35.034631",
     "exception": false,
     "start_time": "2024-11-14T09:57:34.740592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: (2, 10, 128)\n",
      "Attention Ouputs: tf.Tensor(\n",
      "[[[ 0.16796461 -0.29680312 -0.36854327 ...  0.1213422  -0.01294411\n",
      "    0.20422964]\n",
      "  [ 0.06026412 -0.41810444 -0.45633212 ...  0.41323772  0.15755515\n",
      "    0.12214798]\n",
      "  [ 0.59621954 -0.20482707 -0.26585236 ... -0.00865346  0.6714901\n",
      "    0.10502053]\n",
      "  ...\n",
      "  [-0.11432634 -0.43540606 -0.30294836 ...  0.5691043  -0.3535357\n",
      "    0.5544609 ]\n",
      "  [ 0.22376566 -0.22547275  0.07857489 ...  0.20396392 -0.2593416\n",
      "   -0.01794769]\n",
      "  [-0.19177414 -0.34396467 -0.22086892 ...  0.42266598 -0.1686663\n",
      "   -0.06621613]]\n",
      "\n",
      " [[-0.0271943   0.890152    0.06651966 ...  0.03335216 -0.06357548\n",
      "    0.14888097]\n",
      "  [ 0.09474072  0.36804473  0.23109931 ... -0.5623231   0.21032071\n",
      "    0.3945907 ]\n",
      "  [ 0.23362786  0.18425468 -0.06610259 ...  0.1196131   0.09615794\n",
      "   -0.48757905]\n",
      "  ...\n",
      "  [ 0.7888516  -0.16088219  0.20459506 ... -0.03058572 -0.21484427\n",
      "   -0.5165455 ]\n",
      "  [ 0.33547083 -0.35500762  0.20217553 ... -0.6294464   0.10174162\n",
      "   -0.53016406]\n",
      "  [ 0.49657062 -0.08742133  0.18852007 ... -0.70210344  0.29431233\n",
      "   -0.00490079]]], shape=(2, 10, 128), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'multi_head_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the input tensor dimensions\n",
    "d_model = 128  # Model dimensionality\n",
    "num_heads = 8  # Number of attention heads\n",
    "seq_len = 10   # Sequence length\n",
    "\n",
    "# Sample query, key, and value tensors\n",
    "query = tf.random.normal((2, seq_len, d_model))  # (batch_size=2, seq_len, d_model)\n",
    "key = tf.random.normal((2, seq_len, d_model))    # (batch_size=2, seq_len, d_model)\n",
    "value = tf.random.normal((2, seq_len, d_model))  # (batch_size=2, seq_len, d_model)\n",
    "\n",
    "\n",
    "# Create the Multi-Head Attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "attention_output = mha(value, key, query)\n",
    "\n",
    "print(\"Attention Output Shape:\", attention_output.shape)  # (batch_size, seq_len, d_model)\n",
    "print(\"Attention Ouputs:\", attention_output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab67dc",
   "metadata": {
    "papermill": {
     "duration": 0.002223,
     "end_time": "2024-11-14T09:57:35.039625",
     "exception": false,
     "start_time": "2024-11-14T09:57:35.037402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.800382,
   "end_time": "2024-11-14T09:57:36.567332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-14T09:57:15.766950",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
