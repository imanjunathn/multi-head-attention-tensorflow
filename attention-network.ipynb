{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Dense\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T14:52:17.081520Z","iopub.execute_input":"2024-10-23T14:52:17.082078Z","iopub.status.idle":"2024-10-23T14:52:32.242450Z","shell.execute_reply.started":"2024-10-23T14:52:17.081962Z","shell.execute_reply":"2024-10-23T14:52:32.241281Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention,self).__init__(**kwargs)\n    \n    def call(self, q, k, v, mask=None):   \n        dk = tf.cast(tf.shape(k)[-1],tf.float32)\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n        \n        scores = matmul_qk / tf.math.sqrt(dk)\n        \n        if mask is not None:\n            scores += (-1e9 * mask)\n        \n        weights = tf.nn.softmax(scores)\n            \n        return tf.matmul(weights, v)\n        \n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T14:52:32.244562Z","iopub.execute_input":"2024-10-23T14:52:32.245257Z","iopub.status.idle":"2024-10-23T14:52:32.255141Z","shell.execute_reply.started":"2024-10-23T14:52:32.245209Z","shell.execute_reply":"2024-10-23T14:52:32.253811Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(Layer):\n    def __init__(self, d_model, number_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.d_model = d_model\n        self.num_heads = number_heads\n        self.attention = DotProductAttention()  # Scaled dot product attention\n        \n        assert self.d_model % self.num_heads == 0, \"d_model must be divisible by number_heads\"\n        \n        self.depth = self.d_model // self.num_heads\n        \n        self.wq = Dense(self.d_model)\n        self.wk = Dense(self.d_model)\n        self.wv = Dense(self.d_model)\n        self.dense = Dense(self.d_model)\n        \n    def  reshape_tensor(self,x, batch_size, flag=False):\n        if flag:\n            x = tf.reshape(x,(batch_size, -1, self.num_heads, self.depth))\n            return tf.transpose(x,perm=(0,2,1,3))\n        else:    \n            x = tf.transpose(x,perm=(0,2,1,3))\n            x = tf.reshape(x, (batch_size, -1, self.d_model))\n            return x\n    \n    def call(self, q, k, v, mask=None):\n        batch_size = tf.shape(q)[0]\n        \n        wq = self.wq(q)\n        wk = self.wq(k)\n        wv = self.wq(v)\n        \n        q = self.reshape_tensor(wq,batch_size,True)\n        k = self.reshape_tensor(wk,batch_size,True)\n        v = self.reshape_tensor(wv,batch_size,True)\n        \n        scaled_dot_product = self.attention(q, k, v)\n        \n        output = self.reshape_tensor(scaled_dot_product, batch_size)\n        \n        return self.dense(output)\n        \n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T14:52:32.256909Z","iopub.execute_input":"2024-10-23T14:52:32.257331Z","iopub.status.idle":"2024-10-23T14:52:32.274931Z","shell.execute_reply.started":"2024-10-23T14:52:32.257285Z","shell.execute_reply":"2024-10-23T14:52:32.273424Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Define the input tensor dimensions\nd_model = 128  # Model dimensionality\nnum_heads = 8  # Number of attention heads\nseq_len = 10   # Sequence length\n\n# Sample query, key, and value tensors\nquery = tf.random.normal((2, seq_len, d_model))  # (batch_size=2, seq_len, d_model)\nkey = tf.random.normal((2, seq_len, d_model))    # (batch_size=2, seq_len, d_model)\nvalue = tf.random.normal((2, seq_len, d_model))  # (batch_size=2, seq_len, d_model)\n\n\n# Create the Multi-Head Attention layer\nmha = MultiHeadAttention(d_model, num_heads)\n\n# Forward pass\nattention_output = mha(value, key, query)\n\nprint(\"Attention Output Shape:\", attention_output.shape)  # (batch_size, seq_len, d_model)\nprint(\"Attention Ouputs:\", attention_output)  ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T14:52:41.782234Z","iopub.execute_input":"2024-10-23T14:52:41.782725Z","iopub.status.idle":"2024-10-23T14:52:41.894255Z","shell.execute_reply.started":"2024-10-23T14:52:41.782677Z","shell.execute_reply":"2024-10-23T14:52:41.892880Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Attention Output Shape: (2, 10, 128)\nAttention Ouputs: tf.Tensor(\n[[[ 0.03564724 -0.13026899 -0.2905817  ...  0.09156609  0.31391734\n    0.7958979 ]\n  [ 0.0491679   0.11829799 -0.813233   ...  0.59438455  0.33297265\n    0.45926914]\n  [-0.24264418  0.14396743 -0.41736877 ... -0.15397848  0.2737037\n    0.23678291]\n  ...\n  [-0.3663321   0.11688503 -0.95523506 ... -0.03410807  0.5397025\n    0.3563132 ]\n  [-0.30889088  0.38681743 -0.62830323 ... -0.08417644 -0.01798632\n    0.6061058 ]\n  [-0.23371796  0.6275364  -0.12398793 ...  0.3999416   0.2280209\n    1.0406909 ]]\n\n [[ 0.67229265 -0.58834255  0.17819001 ...  0.39587396  0.5937276\n   -0.90302455]\n  [ 0.5271286  -0.27466798  0.05163426 ...  0.3766877   0.06273318\n   -0.08011136]\n  [ 0.43079412 -0.72020763 -0.10172926 ...  0.35286143  1.2609713\n   -0.88027924]\n  ...\n  [ 0.86087286 -0.19494893  0.04824557 ...  0.09723626  0.60974616\n   -0.56391335]\n  [-0.286786   -0.71712863  0.22400053 ... -0.25928017  1.113314\n   -0.15178515]\n  [-0.09421124 -0.62170243  0.34689313 ...  0.37347245  0.8374682\n   -0.47050443]]], shape=(2, 10, 128), dtype=float32)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'multi_head_attention_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}